[{"authors":null,"categories":null,"content":"Will Eatherton is located in San Jose, CA.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1614749859,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Will Eatherton is located in San Jose, CA.","tags":null,"title":"Will Eatherton","type":"authors"},{"authors":[],"categories":[],"content":"The Rise of Opaque encryption Resets the Network Security Industry\n{Note this writeup was done in summer of 2017 while I was at Skyport Systems as Founder and VP of Engineering}\nDuring my time at Cisco and Juniper (1999-2013), our marketing departments unveiled many campaigns around the \u0026ldquo;Intelligent Network.\u0026rdquo; To deliver on this message, we kept building products and features that would use stateful views of deep packet data to help reinforce the marketing story about the value of the network. During the same time, a multi-billion-dollar industry grew out of providing visibility into and control of enterprise traffic, including next-generation firewalls, features in routers, IPS/IDS, UTM, web proxies, data loss prevention, anti-virus, and secure web gateways.\nWe knew that while uncommon at the time, that most of those features we were adding would become irrelevant if the day came that all the data flowing through our devices was encrypted, but that seemed a very far off possibility.\nFast forward to 2017. Not only is encryption here with a vengeance, it will soon cause most network traffic to go dark from the point of view of IT departments. This year the threshold was crossed with [now over 50% of web traffic using HTTPS] (https://www.wired.com/2017/01/half-web-now-encrypted-makes-everyone-safer/). We are also seeing the rise of new encryption protocols that do not accommodate man-in-the-middle (MITM) monitoring. As a result, most traffic will speed by, opaque to inspection from the standpoint of IT and security administrators.\nThese MITM boxes (arguably) own one side of each leg in internal data communications (with the definition that the enterprise IT department owns all client/server endpoints in its network) and for years MITM proxies terminated encrypted sessions and re-originated them, in which case everything is visible. However, moving forward with new protocols increasingly they will only be able to gather extremely superficial intelligence from the traffic headers flowing through them in clear text.\nMITM today Before launching into new developments with encryption, here are the main uses of traffic inspection today:\n Malware and botnet detection to contain the lateral spread of malware and reach-out from enterprise to command and control points Authentication/validation of access to critical segments and systems Detecting policy violations related to content and data access Detecting data exfiltration (theft) General visibility and analytics  Over the past two decades, MITM boxes evolved as invisible inspectors (transparent bumps in the wire) in the enterprise network. This model was relatively easy to operationalize and vendors provided a range of product options.\nToday, there are two main scenarios for deploying MITM boxes. The left side of the diagram below shows traditional inspection of traffic between enterprise servers within their perimiter to external points like clients, SaaS and IaaS services. The right side of the diagram shows the trend over the past 5-7 years towards deploying MITM devices inside the enterprise to provide visibility and segment the enterprise internally. For these internal segmentation cases the network security boxes can intercept traffic between servers, or across boundaries that connect servers to databases and storage.\n Man-in-The-Middle locations in Enterprises Today  What is Opaque Crypto Opaque cryptography covers a broad set of developments that has a heavy focus on HTTPS, but includes other protocol stacks also as shown below.\n   Connection Between New Developments     Application\u0026lt;-\u0026gt;Database Mongo, Postgres, etc w TLS   Application\u0026lt;-\u0026gt;Storage SMBv3, NFS4.1 w Kerberized Crypto   Application\u0026lt;-\u0026gt;Application Increased ease of use of TLS Libraries   Client to Server HTTP/2, TLS1.3, QUIC   Iaas to On-Premises Encrypted VPNs with IPSEC    New developments in network traffic encryption\nWithout doing a deep dive on each of these new developments, we can categorize the trends that make it hard to leverage the existing proxy technologies :\n A general trend towards use of Perfect Forward Secrecy (PFS) and acceptance of it as a valid goal for encryption standards. PFS is a property of a key exchange that deliberately prevents after-the-fact decryption and defeats all passive MITM technologies. Increased use of non-SSL/TLS encryption protocols with no accompanying MITM infrastructure. For example, Google\u0026rsquo;s QUIC is an alternative to TLS that vendors are lagging behind (currently unable to support). Multi-path to improve performance. Multi-path makes decryption difficult because a single MITM box only sees a portion of the needed state. Compressed header state and fewer clear text headers. This makes it difficult to determine what traffic to decrypt.  TLS 1.3 does provide a clear text extension (SNI) to communicate a target server\u0026rsquo;s host name. While it seems useful for decryption determination, it can be easily spoofed.   Increased use of certificate and public key pinning, which are hard blocks to traditional transparent MITM.  Traditionally if you read network security vendor documents the response to this issue was that SaaS vendors simply will not risk their own business by pinning. However, the submissiveness of SaaS vendors is changing with a growing number of services that cross consumer and enterprise boundaries, as well as growing number of SaaS vendors that value the trust of the end users over friction with IT admins.   TCP connection re-use (enabled through HTTP/2 and TLS1.3) complicates state machines for MITM devices. Technologies that are crossing enterprise perimeter that were never meant for transparently inserting network security in the middle, for example IPSEC based tunnels.  Reading through the above list, what is interesting is that while it seems reasonable that many of these developments could be countered to some degree by network security vendors, taken as a sum total the level of disruption in such a short time is staggering.\nForces resulting in Opaque Crypto It is frequently called out that the rise of machine learning in past few years is due to the simultaneous occurrence of 3 factors : 1) significant increases in computation, 2) availability of massive data sets, and 3) Improvement in fundamental algorithms.\nThere is a parallel set of forces that are causing a rise of encryption in a form that removes visibility (i.e. Opaque Crypto). These forces which are detailed in following sub-sections are :\n   Consumer web companies now lead definition of Web protocols    The cost of encryption goes to zero    Cloud ripple effects    Force #1 : Consumer web companies now lead definition of Web protocols At one time, vendors like Cisco, working with service providers and enterprise customers, drove the key web and internet standards. The key force driving the technology choices at end of the day was what would sell more routers, switches, and networking security boxes.\nWhile the fact that technology gets driven for economic ends has not changed, the special interests that are now getting prioritized is that of web companies that want to minimize costs, and maximize value to their consumer customers. In the pursuit of pro-consumer profits, web properties focus on characteristics like low latency, high throughput, good user experience, and trust.\nOn the topic of trust, consumer web companies don\u0026rsquo;t respect the MITM model that financials so love, because in the consumer arena there is no legitimate reason to insert a MITM (only bad reasons like stealing credit card numbers). Additionally in the post Edward Snoweden era, the general negative public reaction to web companies supporting snooping of any kind has added additional pressure on these companies to resist incept models. The result is that web companies believe to earn trust it requires strong end-end security models, and in the pursuit of profit, that is what consumer web companies are delivering.\nA good example of consumer web companies' hostility towards MITM architecture is a 2017 paper that includes authors from Google and Cloudflare that essentially condemns HTTPS interception.\n Paper condemning HTTPS Intercept  This paper is the result of a massive investment. The authors tested a vast combination of security products and software versions from top vendors, and offer this finding in summary:\n Most concerningly, 62% of traffic that traverses a network middlebox has reduced security and 58% of middlebox connections have severe vulnerabilities. We investigated popular antivirus and corporate proxies, finding that nearly all reduce connection security and that many introduce vulnerabilities\n After publication of this paper, a US agency issued a CERT (Alert TA17-075A) citing it and issuing a similar warning about the risks of HTTPS interception.\nIn recent discussions with a household name financial about these trends, the customer noted that they were talking to Google about the disruption that standards like Google\u0026rsquo;s QUIC/TLS1.3 could cause across their industry due to these standards' implicit push against interception. While the frustration is understandable given that financials are heavily invested in the current model for monitoring/protecting their perimeter, the new direction looks clear barring major government interference.\nForce #2 : The cost of encryption goes to zero Though not strictly a cause, a drop in the cost of encryption was necessary to ease its adoption. Improvements to key encryption standards in hardware\u0026ndash;and more importantly software\u0026ndash;have made it easier to support very strong encryption algorithms, and at a level that performance is no longer an issue for either the client or server side of a connection.\nImprovements like Intel Advanced Encryption Standard instructions speed up encryption, and there are ongoing efforts to improve all aspects of TLS performance, including making TLS a kernel service.\nForce #3 : Cloud ripple effects This force was harder to pin down, but outside of web traffic and the falling cost of encryption there was clearly at least one more force driving the rise of opaque encryption. Two example trends that are not explained by the two forces above are the recent moves towards encrypted storage traffic with NFS4.1 using kerberized encryption as well as increased use of SMB 3.0 with encryption.\nSo why is the above happening ? Lets start with the assumption that data and workloads without notable business value will continue quickly moving to public cloud. If you are an enterprise vendor of ISV software or systems, you are flailing around, looking to counter this trend with new features, and to push the notion that your customers should be keeping important stuff in a enterprise owned data centers. This leads to features like encrypted data at rest, as well as encrypted data traffic for storage, databases and applications.\nBeyond these vendor driven features, cloud in the form of SaaS has produced thousands of enterprise SaaS services. Beyond client connections, these SaaS services most often involve agents or VMs sitting in the enterprise data center that reach out to their cloud service origin (most commonly on AWS) to make some sort of data connection and do data integrations. These SaaS services are following in the foot steps of their consumer web brethren and all will be increasingly opaque.\nFinally going to the core trend of IaaS adoption in enterprise, services like AWS are resulting in many (poorly managed) enterprises sprinkling VPC VPN gateways all over their internal network. Each of these end points is rendering the outer perimeter of the enterprise meaningless as each is a bi-directional gateway between an entire world encapsulated in the AWS VPC to the heart of the data center. And again these are all encrypted traffic that is not being proxied by those HTTPS interception MITM devices.\nEnterprise vendors' responses to opaque crypto The network security industry has largely been quiet about the trends discussed here. Some companies like Bluecoat have been visible in their struggles to keep up with protocols like TLS1.3. But overall there has been little substantive industry-wide discussion of the issues or possible solutions.\nOne exception is Cisco, which has highlighted several issues with opaque crypto and advocated a potential solution. In mid-2017 Cisco introduced Cisco Encrypted Traffic Analytics, and they deserve credit for being one of the only established security vendors to call out this issue. They also deserve credit for publishing academic papers related to their solutions and making available software for feature extraction from packet traces, plus an analysis tool on github that seem related to their for-profit product.\nSo what does Cisco propose? As their graphic below shows, first they look at clear text TLS information (also known as TLS fingerprinting). This can be as simple as the order of presentation of available bulk encryption algorithms, but of course is usually more complex. Also, they offer nice real time data collection of the TLS packet streams across a variety of extracted features, including old favorites like packet size and inter-packet gap.\n Cisco Encrypted Analytics  But while Cisco deserves credit, there seem to be fundamental issues with their offering.\n The proposal focuses on malware detection, which overlooks other goals of MITM proxies such as policy violations related to data access, detecting data exfiltration, and validating/authenticating access to management segments and systems. As mentioned above, TLS 1.3 and other new protocols are becoming more opaque. Cisco\u0026rsquo;s approach will likely become less relevant with the reduced clear text available with TLS1.3, the smaller set of supported cipher suites, all making TLS fingerprinting more difficult. Cisco\u0026rsquo;s solution seems to focus on TLS and omit technologies like IPSEC VPNs, Kerberized file transfer protocols, and QUIC. In the past, solutions like malware detonation were effective until bad actors learned how to do end runs around the technology. By extension, it seems likely that once hackers see the solution, they will learn to avoid TLS fingerprinting, matching packet sizes and gaps, and other \u0026ldquo;tells\u0026rdquo; to appear similar to known applications. Cisco\u0026rsquo;s solution today depends on data collection using Cisco technology collector points.  Despite these issues, Cisco\u0026rsquo;s proposal adds value. It would help to know what companies like Palo Alto Networks, Bluecoat, F5, and Checkpoint have to say on this topic.\nAs a final note, back in 2012 an IETF presentation by Cisco Fellow David McGrew pointed out many of the issues with HTTPS interception discussed here. Since that time, the new trends have only pushed it further off the cliff.\nThe shape of the future What do the next few years hold, and what are practical solutions for enterprise-wide visibility and policy enforcement?\n The transition to opaque crypto will start in earnest in 2018, but will take years to complete. Mini-industries will spring up to keep legacy protocols alive for the finance industry for another 10 years. But for most enterprises, simple to deploy central policy visibility and enforcement will go away and companies will adapt. There might be a rise of non-transparent proxies for client endpoints in the enterprise. But non-transparent proxies are known to be relatively difficult to maintain, so it is unclear if this will gain widespread adoption. Enterprises may increasingly consider client endpoints to be lost causes, and establish a goal to heavily segment them from business applications. To deal with rogue clients, use of frequent rebuild/replace of client software could be more common. For applications running on server endpoints in enterprise data centers, there will be an ongoing trend towards applying policy to the application instead of the network. Policy (for example, what entities this application can talk to or receive communication from), will increasingly be a part of a developer\u0026rsquo;s definition of an application\u0026ndash;similar to software dependencies, data models, and API definitions. 1  The trend towards opaque encryption will produce a stronger model for secure communication. But while the networking industry resets, vendors and enterprises will struggle to find the new normal.\n  Skyport Systems was acquired by Cisco Systems in early 2018. \u0026#x21a9;\u0026#xfe0e;\n   ","date":1504648829,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614749859,"objectID":"7eef7cb7b5b92643a18fce8584183d16","permalink":"https://willeatherton.com/post/crypto1v4/","publishdate":"2017-09-05T14:00:29-08:00","relpermalink":"/post/crypto1v4/","section":"post","summary":"The Rise of Opaque encryption Resets the Network Security Industry\n{Note this writeup was done in summer of 2017 while I was at Skyport Systems as Founder and VP of Engineering}","tags":["IT"],"title":"Opaque Crypto : What happens when the Enterprise Network goes Dark ?","type":"post"},{"authors":[],"categories":[],"content":"{Note this writeup was done privately at end of 2012 and so pretty outdated, but still I think it was a decent summary of the state of ARM at that time and so I publish it for fun. I am considering to do an updated Arm Analysis given the changes that have happened in past year}\nWill Eatherton - willeatherton@gmail.com\nTable of Contents  Exec Summary Challenges for ARM as Server CPU ISA  OS Support Optimizations Compilation Tool Chains Hypervisor Support Java Virtual Machines   CPU level Analysis of ARM vs x86 Analysis of Arm CPU for a M\u0026amp;R Compute Node    Exec Summary The buzz around Arm based CPU\u0026rsquo;s for the Server Ecosystem has been growing notably in mid 2012. Gartner predicts that ARM servers will own 15 percent of the server CPU market within four years.\nThere are numerous planned ARM based CPU server chips for server market : ST Micro , Applied Micro has early 64-bit prototype and seems to be leading support for ARM v8, [Marvell] (http://www.marvell.com/embedded-processors/armada-xp/) was early with a 32-bit server ARM based server focused processor, AMD has announced 64-bit ARM CPUs by 2014 , LSI , and startup Calxeda. Additionally there are indications within the industry that IBM/Samsung may be considering opportunities in server space using ARM (maybe more as SOC designs with partners).\n  Beyond silicon plays, early stage startups like [Netspeed] (http://www.netspeedsystems.com) are looking at enabling the growing number of players interested in high end multi-core ARM SOCs by providing a coherent interconnect\n  [ARM is also working on a coherent interconnectt] (http://www.arm.com/about/newsroom/arm-announces-new-high-performance-system-ip-to-address-demand-for-energy-efficient-many-core.php) for massively multi-core designs with support for 16 cores going to 32 in the future\n  From a systems standpoint Dell has announced an ARM based server starting with a 32-bit version, and [HP has indicated] (http://liliputing.com/2012/06/hp-project-moonshot-drops-arm-will-use-intel-atom-servers.html) that their proof of concept project (Moonshot) will support ATOM and ARM based CPUs.\n  Beyond the system level end Mega Data center operators like [Facebook have also have shown interestt] (http://www.eetimes.com/electronics-news/4400339/Linaro-ARM-server-efforts-targets-Linux-code) in helping to support ARM insertion into data center\n  Note that for system vendors as well as DIY Hyperscale DC integrators like Facebook, support of ARM in DC is somewhat self serving for the basic reason of providing negotiating leverage with Intel on pricing\n  Looking at specific application of a Map and Reduce focused compute node, the question is what an ARM cpu based server could look like in 2014/2015 and if it would add any notable differentiation benefits Vs Intel. The analysis below focuses on exploring implications of 64-bit ARM cores used in data center for M\u0026amp;R applications.\nConclusion : The analysis below concludes that ARM based servers for the target application do not seem worth the risks at this point. If future projected ARM solutions could show 3-5x increase in final performance after all factors (including software) in same power profile then it would be worth revisiting.\nChallenges for ARM as Server CPU ISA Before getting into a low level performance/hardware analysis of ARM as a server CPU, it is important to look at what the implications of replacing x86 in data center with ARM ISA will have on the ecosystem for Software/System architecture.\nOS Support Optimizations There is clearly gaps today for strong generalized Linux support of the upcoming ARM silicon for DC. There is a recent initiative termed [Linaro to focus on flushing out full linux support for ARM] (http://www.eetimes.com/electronics-news/4400339/Linaro-ARM-server-efforts-targets-Linux-code) related to topics starting with boot sequences and going from there. Beyond base driver level support a major area of concern from performance oriented software developers is the extensive tooling around benchmarking and tuning of Linux on x86 that will have to mature with ARM.\nCompilation Tool Chains It has been common wisdom for a number of years that Intel\u0026rsquo;s optimizing compiler (ICC) for C/C++ is one of the better compilers in the industry and represents again the level of multi-year optimization around x86. [Recent studies in 2012] (http://www.behardware.com/articles/847-1/the-impact-of-compilers-on-x86-x64-cpu-architectures.html), there continues to be seen a notable benefit of ICC compilers of other alternatives like GCC and Microsoft. From the perspective of Hadoop (a Java application) the impact of not having the Intel C/C++ compiler from performance/memory standpoint may not be major, but is representative of the types of optimization issues that ARM will have entering wide spread data center use.\nHypervisor Support With ARMv7/v8 there is some support for Hardware and I/O virtualization that will be supported and available in silicon by 2014. Paravirtualization is the description of the technique used to present software interfaces up to virtual machines and is used to compensate for these gaps (accomplished by intercepting certain instructions from guest OS and interpreting them differently for the actual hardware) .\n Note the ongoing maintenance of para-virtualization can be difficult with consistent performance and reliability  [Vmware has indicated pretty bluntly] (http://www.wired.com/wiredenterprise/2012/08/vmware-on-arm/) they are in no rush to support ARM, this is of course a major issue for adoption of ARM in the data center as the open source solutions will be only real option for virtualization for ARM servers in the foreseeable future.\nFor [KVM support of ARM] (http://blog.xen.org/index.php/2012/09/21/xensummit-sessions-new-pvh-virtualisation-mode-for-arm-cortex-a15arm-servers-and-x86/) which seem to be making progress but still early phase and have many years of work ahead of them on topics related to performance tuning, benchmarking, and para-virtualization enhancements to better compensate for missing virtualization extensions and hardware in ARM v7/v8.\n For the x86 architecture (Intel and AMD) starting providing support for virtualization many years ago and the time frame for ARM to reach parity will be a long time  From the perspective of a Hadoop cluster, the weak support of hypervisor support on ARM is not necessarily a show stopper as it is a common case to run the Hadoop stack a non virtualized OS running bare metal, but it is still representative of the types of optimization issues that ARM will have entering wide spread data center use.\nJava Virtual Machines As with prior topics, the immature state of JVM optimization for ARM compared to x86 will again be an impediment for rapid ARM adoption. While in general the performance analysis data is sketchy in this area, [some example experiments] (http://fullshovel.wordpress.com/2012/07/11/java-vs-c-on-arm/ ) in 2012 have shown that with OpenJDK performance between C and Java on x86 platforms are in ballpark of 1:1, but when the same benchmarks are run on ARM the ratios can be as high as 3.6x to 8.9x worse for ARM. This indicates that the level of tuning around ARM JVM support is still very immature.\nOracle [has recently announced] (https://blogs.oracle.com/henrik/entry/oracle_releases_jdk_for_linux) that they will support their JRE on ARM. This is very important is the Oracle JRE is commonly viewed as the clear industry grade/performance leader for JRE support compared to alternative commercial and open source options. There are some functional limitations in Oracles planned support of ARM, but they do not appear to have major impact on server applications. However, there is no benchmarking data yet available for Oracle\u0026rsquo;s JRE and it is expected there will be a multi-year evolution required. Additionally the first port is focused on 32-bit and ARM v7, so support for the new set of 64-bit cores will not be until well into 2013.\nThe JVM support for ARM is key to Hadoop which is Java based.\nCPU level Analysis of ARM vs x86 Based on discussion with a processor Architect, interesting data points :\n  Expects to see a 32-core, 64-bit core devices in prototype by 2H 2013\n  for integer/text manipulations (common in M\u0026amp;R applications which is example application considered here) expects that first order it can be approximated that ARM v8 cores should be on par with x86 E5 cores at the machine code level (ignoring any virtualization or tool chain differences that were explored above) at the same clock\n  From power standpoint expects 32 core 64-bit ARM v8 core CPU to be similar power as 10 core x86 CPU in similar time frame (100W) for same system level functionality. This implies an upper bounds of 3x benefit per core for ARM\n Industry discussion of the back of the envelope statistics for a 5-10x delta between power of x86 server cores and ARM cores, generally are comparing Arm V6 32-bit cores which do not represent the power per core when the ARM ISA moves to 64-bit and starts adding more overheads like full floating point support, virtualization support (e.g. nested page tables), and coherent interconnect overheads.    He does not see that coherent interconnect of 32-cores will be bottleneck based on his analysis of ARM\u0026rsquo;s recent multi-interconnect (CN-504)\n  Looking beyond a single CPU, There is not a concrete plan available yet about support for multi-CPU mesh configurations like Intel\u0026rsquo;s QPI connection for larger shared memory complex\n  The implication of not having this multi-CPU configuration is that each silicon instance is a standalone CPU without ability to leverage shared memory and require finer grain segregation of tasks across CPUs with separate distributed application instances across each CPU\n  There has been a [statement from Applied micro CEO] (http://www.theregister.co.uk/2012/08/30/applied_micro_x_gene_server_chip) that in future up to 1024 cores across 64 CPU\u0026rsquo;s is planned, though there is not much additional detail on this yet.\n   Example Intel E5 Multi-CPU Configuration  Analysis of Arm CPU for a M\u0026amp;R Compute Node At a system level beyond the topic of the potential compute benefits (per Watt) of ARM vs x86, and the software complexities is question of how relevant this trade-off for a given application area.\nFirst lets consider a very rough estimate for normalizing performance to BIPS (Billions of Instructions per Second) for integer operations of an ARM and x86 based CPU within 100W budget for silicon expected to be available by end of 2013. Note that the relative JVM performance estimates may be optimistic in favor of ARM.\nRelative Comparison of performance for x86 Vs ARM 64-bit Server CPU\n   Performance Factor ARM x86     Number of Cores 32 10   Clock (Ghz) 3 2.4   ISA efficiency for Integer operations .5 1   Linux OS Perf Relative to x86 .95 1   JVM Performance Implications .7 1   Final relative BIPS 32 24    In summary while a crude estimate, this final ratio of relative performance with in 100W is close enough to 1:1 that it is not interesting.\nGoing to system level, if we assume that over time with software and further silicon optimization ARM based CPUs improved to a solid 2x final performance (relative BIPS) per watt benefit of ARM over x86 after all overheads, how much would this impact system optimization for the Map and Reduce application in a rack server ?\n  Taking into account memory, disk, IO and other overheads, the final system impact in system density for a 2:1 would have estimated \u0026lt;20% benefit at system level. This does not seem to have enough impact to warrant the significant risk and effort.\n  As an example, consider that for recent hadoop cluster analysis, the ratio of x86 cores to spindles may be as high as 1:5, this implies that a blade with say 32 cores would matchup with 160 spinning disks. This is significant amount of space and power compared to the CPU complex, making the raw CPU performance less relevant.\n  If it were possible to achieve a ratio of say 3-5x of ARM over x86 in final relative BIP performance per watt, and aiming for 100\u0026rsquo;s of ARM cores on a blade (or in a 2RU rack server), then merging this optimization with a major overhaul of the system design to match the massively multi-core architecture in areas of disk/memory/IO could result in a significant different optimization point then x86 servers today.\n","date":1346882429,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614749859,"objectID":"46cdaa60189eb6ef898e6157d90004ce","permalink":"https://willeatherton.com/post/armanalysis/","publishdate":"2012-09-05T14:00:29-08:00","relpermalink":"/post/armanalysis/","section":"post","summary":"{Note this writeup was done privately at end of 2012 and so pretty outdated, but still I think it was a decent summary of the state of ARM at that time and so I publish it for fun.","tags":["tag1"],"title":"ARM CPU's for Data Center Servers","type":"post"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614731196,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://willeatherton.com/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]